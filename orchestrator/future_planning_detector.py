
from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from typing import Optional, List, TYPE_CHECKING

from openai import AsyncOpenAI

if TYPE_CHECKING:
    from config.settings import Settings

logger = logging.getLogger(__name__)


@dataclass
class FuturePlanningResult:
    is_future_planning: bool
    detected_plan: str
    detected_datetime: Optional[str]
    confidence: float
    reason: str


class FuturePlanningDetector:

    def __init__(
        self,
        openai_client: AsyncOpenAI,
        settings: "Settings",
    ):
        self._client = openai_client
        self._settings = settings
        self._model = getattr(settings, "AGENTS_MODEL", "gpt-4o-mini")
        logger.info("future_planning_detector:init:success")

    async def detect(
        self,
        message: str,
        sender_id: str,
        recipient_id: str,
        context: Optional[List[str]] = None,
    ) -> FuturePlanningResult:
        try:
            result = await self._llm_analysis(message, context)
            
            logger.info(
                "future_planning_detector:result",
                extra={
                    "sender_id": sender_id,
                    "recipient_id": recipient_id,
                    "message_preview": message[:50],
                    "is_future_planning": result.is_future_planning,
                    "confidence": result.confidence,
                    "detected_plan": result.detected_plan[:100] if result.detected_plan else "",
                },
            )
            
            return result
            
        except Exception as e:
            logger.error(
                "future_planning_detector:error",
                extra={
                    "sender_id": sender_id,
                    "recipient_id": recipient_id,
                    "error": str(e),
                },
                exc_info=True,
            )
            
            return FuturePlanningResult(
                is_future_planning=False,
                detected_plan="",
                detected_datetime=None,
                confidence=0.0,
                reason="llm_error",
            )

    async def _llm_analysis(
        self,
        message: str,
        context: Optional[List[str]] = None,
    ) -> FuturePlanningResult:
        context_text = ""
        if context:
            context_text = "\n".join(f"- {m}" for m in context[-5:])
            context_text = f"\n\nÙ¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ:\n{context_text}"
        
        system_prompt = """ØªÙˆ ÛŒÚ© ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù¾ÛŒØ§Ù… Ù‡Ø³ØªÛŒ. ÙˆØ¸ÛŒÙÙ‡â€ŒØ§Øª ØªØ´Ø®ÛŒØµ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡ Ø§Ø³Øª.

Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡ ÛŒØ¹Ù†ÛŒ:
- Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø§Ù†Ø¬Ø§Ù… Ú©Ø§Ø±ÛŒ Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡ (ÙØ±Ø¯Ø§ØŒ Ù‡ÙØªÙ‡ Ø¨Ø¹Ø¯ØŒ ...)
- Ø¯Ø¹ÙˆØª Ø¨Ù‡ Ù…Ù„Ø§Ù‚Ø§Øª ÛŒØ§ Ù‚Ø±Ø§Ø±
- Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø±ÙˆÛŒØ¯Ø§Ø¯
- Ø³ÙˆØ§Ù„ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡
- Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù‡Ù…Ø±Ø§Ù‡ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ú©Ø§Ø±ÛŒ

Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ:
âœ… "ÙØ±Ø¯Ø§ Ø¨Ø±ÛŒÙ… Ú©ÙˆÙ‡ØŸ"
âœ… "Ù‡ÙØªÙ‡ Ø¨Ø¹Ø¯ ÙˆÙ‚Øª Ø¯Ø§Ø±ÛŒ Ù†Ø§Ù‡Ø§Ø± Ø¨Ø±ÛŒÙ…ØŸ"
âœ… "Ø´Ù†Ø¨Ù‡ Ø³Ø§Ø¹Øª Ûµ Ø¨ÛŒØ§ Ù¾ÛŒØ´Ù…"
âœ… "Ú©ÛŒ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒÙ… Ù‡Ù…Ø¯ÛŒÚ¯Ø±Ùˆ Ø¨Ø¨ÛŒÙ†ÛŒÙ…ØŸ"
âœ… "Ø¨Ø±ÛŒÙ… Ø³ÛŒÙ†Ù…Ø§"
âœ… "Ù…ÛŒØ§ÛŒ ÙÙˆØªØ¨Ø§Ù„ØŸ"
âœ… "ÛŒÙ‡ Ù‚Ø±Ø§Ø± Ø¨Ø°Ø§Ø±ÛŒÙ…"

Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØºÛŒØ± Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ:
âŒ "Ø¯ÛŒØ±ÙˆØ² Ø±ÙØªÙ… Ú©ÙˆÙ‡" (Ú¯Ø°Ø´ØªÙ‡)
âŒ "Ø³Ù„Ø§Ù…ØŒ Ø®ÙˆØ¨ÛŒØŸ" (Ø§Ø­ÙˆØ§Ù„Ù¾Ø±Ø³ÛŒ)
âŒ "Ú†ÛŒÚ©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒØŸ" (Ø³ÙˆØ§Ù„ Ø¹Ù…ÙˆÙ…ÛŒ)
âŒ "Ù…Ù…Ù†ÙˆÙ† Ø§Ø² Ú©Ù…Ú©Øª" (ØªØ´Ú©Ø±)
âŒ "Ø®ÙˆØ´ Ú¯Ø°Ø´Øª" (Ú¯Ø°Ø´ØªÙ‡)
âŒ "Ú†Ù‡ Ø®Ø¨Ø±ØŸ" (Ø§Ø­ÙˆØ§Ù„Ù¾Ø±Ø³ÛŒ)

Ù¾Ø§Ø³Ø® ÙÙ‚Ø· Ø¨Ù‡ ÙØ±Ù…Øª JSON Ø¨Ø¯Ù‡:
{
    "is_future_planning": true/false,
    "detected_plan": "Ø®Ù„Ø§ØµÙ‡ Ú©ÙˆØªØ§Ù‡ Ø¨Ø±Ù†Ø§Ù…Ù‡ (Ù…Ø«Ù„Ø§Ù‹: Ø±ÙØªÙ† Ø¨Ù‡ Ú©ÙˆÙ‡) ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø§Ú¯Ø± Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ù†ÛŒØ³Øª",
    "detected_datetime": "Ø²Ù…Ø§Ù† ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ (Ù…Ø«Ù„Ø§Ù‹: ÙØ±Ø¯Ø§) ÛŒØ§ null",
    "confidence": 0.0-1.0,
    "reason": "Ø¯Ù„ÛŒÙ„ Ú©ÙˆØªØ§Ù‡"
}"""

        user_prompt = f"""Ù¾ÛŒØ§Ù… Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù†:{context_text}

Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯:
"{message}"

Ø¢ÛŒØ§ Ø§ÛŒÙ† ÛŒÚ© Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒØ±ÛŒØ²ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡ Ø§Ø³ØªØŸ"""

        response = await self._client.chat.completions.create(
            model=self._model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.1,
            max_tokens=300,
            response_format={"type": "json_object"},
        )
        
        content = response.choices[0].message.content or "{}"
        
        try:
            data = json.loads(content)
        except json.JSONDecodeError as e:
            logger.error(
                "future_planning_detector:json_parse_error",
                extra={"content": content, "error": str(e)},
            )
            raise
        
        return FuturePlanningResult(
            is_future_planning=data.get("is_future_planning", False),
            detected_plan=data.get("detected_plan", ""),
            detected_datetime=data.get("detected_datetime"),
            confidence=float(data.get("confidence", 0.0)),
            reason=data.get("reason", "llm_analysis"),
        )

    async def generate_acknowledgment_response(
        self,
        detected_plan: str,
        detected_datetime: Optional[str],
        twin_name: Optional[str],
        language: str = "fa",
    ) -> str:
        name_part = twin_name if twin_name else "Ø§ÛŒØ´Ø§Ù†"
        
        if language == "fa":
            return f"Ø¨Ø§Ø´Ù‡ØŒ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø±Ùˆ Ø¨Ù‡ {name_part} Ø§Ø·Ù„Ø§Ø¹ Ù…ÛŒâ€ŒØ¯Ù…. ÙˆÙ‚ØªÛŒ Ø¬ÙˆØ§Ø¨ Ø¯Ø§Ø¯ØŒ ØªÙˆÛŒ Ù¾ÛŒØ§Ù… Ø¨Ø¹Ø¯ÛŒØª Ø¨Ù‡Øª Ù…ÛŒâ€ŒÚ¯Ù… Ú†ÛŒ Ú¯ÙØª. ğŸ‘"
        else:
            return f"Got it! I'll let {name_part} know about this. When they respond, I'll tell you in our next chat. ğŸ‘"
