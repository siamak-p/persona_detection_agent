
from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from typing import Optional, List, TYPE_CHECKING

from openai import AsyncOpenAI

if TYPE_CHECKING:
    from config.settings import Settings
    from db.postgres_financial_threads import FinancialThread, FinancialThreadMessage

logger = logging.getLogger(__name__)


@dataclass
class FinancialDetectionResult:
    is_financial: bool
    topic_summary: str
    amount: Optional[str]
    urgency: str
    confidence: float
    reason: str


@dataclass
class ThreadContinuationResult:
    is_continuation: bool
    is_closure: bool
    confidence: float
    reason: str


class FinancialTopicDetector:

    def __init__(
        self,
        openai_client: AsyncOpenAI,
        settings: "Settings",
    ):
        self._client = openai_client
        self._settings = settings
        self._model = getattr(settings, "AGENTS_MODEL", "gpt-4o-mini")
        logger.info("financial_topic_detector:init:success")


    async def detect(
        self,
        message: str,
        sender_id: str,
        creator_id: str,
        context: Optional[List[str]] = None,
    ) -> FinancialDetectionResult:
        try:
            result = await self._llm_detect_financial(message, context)
            
            logger.info(
                "financial_topic_detector:detect:result",
                extra={
                    "sender_id": sender_id,
                    "creator_id": creator_id,
                    "message_preview": message[:50],
                    "is_financial": result.is_financial,
                    "confidence": result.confidence,
                    "topic_summary": result.topic_summary[:100] if result.topic_summary else "",
                },
            )
            
            return result
            
        except Exception as e:
            logger.error(
                "financial_topic_detector:detect:error",
                extra={
                    "sender_id": sender_id,
                    "creator_id": creator_id,
                    "error": str(e),
                },
                exc_info=True,
            )
            
            return FinancialDetectionResult(
                is_financial=False,
                topic_summary="",
                amount=None,
                urgency="normal",
                confidence=0.0,
                reason="llm_error",
            )

    async def check_continuation(
        self,
        message: str,
        thread: "FinancialThread",
        recent_messages: List["FinancialThreadMessage"],
    ) -> ThreadContinuationResult:
        try:
            result = await self._llm_check_continuation(message, thread, recent_messages)
            
            logger.info(
                "financial_topic_detector:check_continuation:result",
                extra={
                    "thread_id": thread.id,
                    "is_continuation": result.is_continuation,
                    "is_closure": result.is_closure,
                    "confidence": result.confidence,
                },
            )
            
            return result
            
        except Exception as e:
            logger.error(
                "financial_topic_detector:check_continuation:error",
                extra={"thread_id": thread.id, "error": str(e)},
                exc_info=True,
            )
            
            return ThreadContinuationResult(
                is_continuation=False,
                is_closure=False,
                confidence=0.0,
                reason="llm_error",
            )


    async def _llm_detect_financial(
        self,
        message: str,
        context: Optional[List[str]] = None,
    ) -> FinancialDetectionResult:
        context_text = ""
        if context:
            context_text = "\n".join(f"- {m}" for m in context[-5:])
            context_text = f"\n\nÙ¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ:\n{context_text}"
        
        system_prompt = """ØªÙˆ ÛŒÚ© ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù¾ÛŒØ§Ù… Ù‡Ø³ØªÛŒ. ÙˆØ¸ÛŒÙÙ‡â€ŒØ§Øª ØªØ´Ø®ÛŒØµ Ø¨Ø­Ø«â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ø§Ø³Øª.

âš ï¸ Ù‚Ø§Ø¹Ø¯Ù‡ Ù…Ù‡Ù…: Ù‡Ø± Ø³ÙˆØ§Ù„ÛŒ Ú©Ù‡ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù¾ÙˆÙ„ØŒ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒØŒ Ú©Ø±ÛŒÙ¾ØªÙˆØŒ Ø³Ù‡Ø§Ù…ØŒ Ø§Ø±Ø²ØŒ ÛŒØ§ Ù…Ø¹Ø§Ù…Ù„Ù‡ Ø¨Ø§Ø´Ø¯ = Ù…Ø§Ù„ÛŒ Ø§Ø³Øª!

Ø¨Ø­Ø«â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ø´Ø§Ù…Ù„:
- Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù¾ÙˆÙ„ Ù‚Ø±Ø¶ Ø¯Ø§Ø¯Ù†/Ú¯Ø±ÙØªÙ†
- Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…Ø¹Ø§Ù…Ù„Ù‡ ÛŒØ§ Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´ Ù‡Ø± Ú†ÛŒØ²ÛŒ
- Ø¨Ø­Ø« Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø¨Ø¯Ù‡ÛŒ ÛŒØ§ Ø·Ù„Ø¨
- Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ù…Ø´Ø§Ø±Ú©Øª Ù…Ø§Ù„ÛŒ
- Ú©Ø±ÛŒÙ¾ØªÙˆÚ©Ø§Ø±Ù†Ø³ÛŒ Ùˆ Ø§Ø±Ø² Ø¯ÛŒØ¬ÛŒØªØ§Ù„ (Ø¨ÛŒØªâ€ŒÚ©ÙˆÛŒÙ†ØŒ Ø§ØªØ±ÛŒÙˆÙ…ØŒ ØªØªØ±ØŒ Ø¯ÙˆØ¬â€ŒÚ©ÙˆÛŒÙ† Ùˆ...)
- Ø¨ÙˆØ±Ø³ Ùˆ Ø³Ù‡Ø§Ù… (Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´ Ø³Ù‡Ù…ØŒ ETFØŒ ØµÙ†Ø¯ÙˆÙ‚)
- Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ø±Ø²ÛŒ (Ø¯Ù„Ø§Ø±ØŒ ÛŒÙˆØ±Ùˆ)
- Ù‡Ø± Ø³ÙˆØ§Ù„ÛŒ Ú©Ù‡ Ø¨Ø®ÙˆØ§Ø¯ Ø¨ÙÙ‡Ù…Ù‡ Ø·Ø±Ù Ø¹Ù„Ø§Ù‚Ù‡â€ŒÙ…Ù†Ø¯ Ø¨Ù‡ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù‡Ø³Øª ÛŒØ§ Ù†Ù‡
- Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ ÙØ±ÙˆØ´ ÛŒØ§ Ø®Ø±ÛŒØ¯ Ú†ÛŒØ²ÛŒ Ø¨Ù‡ Ø·Ø±Ù Ù…Ù‚Ø§Ø¨Ù„

ğŸš¨ Ø§ÛŒÙ† Ù…ÙˆØ§Ø±Ø¯ Ø­ØªÙ…Ø§Ù‹ Ù…Ø§Ù„ÛŒ Ù‡Ø³ØªÙ†Ø¯ (Ø­ØªÛŒ Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ø´Ø®ØµÛŒ Ø¨Ù‡ Ù†Ø¸Ø± Ø¨Ø±Ø³Ù†Ø¯):
âœ… "Ø¨ÛŒØªâ€ŒÚ©ÙˆÛŒÙ† Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒØŸ" â†’ Ù…Ø§Ù„ÛŒ (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´)
âœ… "Ø±ÙˆÛŒ Ø¨ÛŒØªâ€ŒÚ©ÙˆÛŒÙ† Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØŸ" â†’ Ù…Ø§Ù„ÛŒ (Ø³ÙˆØ§Ù„ Ø¯Ø±Ø¨Ø§Ø±Ù‡ ØªÙ…Ø§ÛŒÙ„ Ø¨Ù‡ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ)
âœ… "Ø§Ø±Ø² Ø¯ÛŒØ¬ÛŒØªØ§Ù„ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒØŸ" â†’ Ù…Ø§Ù„ÛŒ (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø®Ø±ÛŒØ¯)
âœ… "Ø§ØªØ±ÛŒÙˆÙ… Ø¯Ø§Ø±ÛŒØŸ" â†’ Ù…Ø§Ù„ÛŒ (Ø³ÙˆØ§Ù„ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø¯Ø§Ø±Ø§ÛŒÛŒ)
âœ… "Ø³Ù‡Ø§Ù… Ø®Ø±ÛŒØ¯ÛŒØŸ" â†’ Ù…Ø§Ù„ÛŒ
âœ… "ØªÙˆÛŒ Ú©Ø±ÛŒÙ¾ØªÙˆ Ù‡Ø³ØªÛŒØŸ" â†’ Ù…Ø§Ù„ÛŒ
âœ… "Ø¯Ù„Ø§Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒØŸ" â†’ Ù…Ø§Ù„ÛŒ
âœ… "Ø·Ù„Ø§ Ø¨Ø®Ø±Ù…ØŸ" â†’ Ù…Ø§Ù„ÛŒ
âœ… "Ù¾ÙˆÙ„ Ø¯Ø§Ø±ÛŒ Ù‚Ø±Ø¶ Ø¨Ø¯ÛŒØŸ" â†’ Ù…Ø§Ù„ÛŒ

Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØºÛŒØ± Ù…Ø§Ù„ÛŒ:
âŒ "Ú†Ù‚Ø¯Ø± Ø®ÙˆØ´Ø­Ø§Ù„Ù…!" (Ø§Ø­Ø³Ø§Ø³)
âŒ "Ø®ÛŒÙ„ÛŒ ÙˆÙ‚ØªÙ‡ Ù†Ø¯ÛŒØ¯Ù…Øª" (Ø§Ø­ÙˆØ§Ù„Ù¾Ø±Ø³ÛŒ)
âŒ "Ø§Ù…Ø±ÙˆØ² Ú†ÛŒÚ©Ø§Ø± Ú©Ø±Ø¯ÛŒØŸ" (Ø³ÙˆØ§Ù„ Ø¹Ù…ÙˆÙ…ÛŒ)
âŒ "ÙÛŒÙ„Ù… Ø¯ÛŒØ¯ÛŒØŸ" (Ø³Ø±Ú¯Ø±Ù…ÛŒ)

Ù¾Ø§Ø³Ø® ÙÙ‚Ø· Ø¨Ù‡ ÙØ±Ù…Øª JSON Ø¨Ø¯Ù‡:
{
    "is_financial": true/false,
    "topic_summary": "Ø®Ù„Ø§ØµÙ‡ Ú©ÙˆØªØ§Ù‡ (Ù…Ø«Ù„Ø§Ù‹: Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ú©Ø±ÛŒÙ¾ØªÙˆ / Ø³ÙˆØ§Ù„ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ) ÛŒØ§ Ø®Ø§Ù„ÛŒ",
    "amount": "Ù…Ø¨Ù„Øº Ø§Ú¯Ø± Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯ ÛŒØ§ null",
    "urgency": "urgent/normal/low",
    "confidence": 0.0-1.0,
    "reason": "Ø¯Ù„ÛŒÙ„ Ú©ÙˆØªØ§Ù‡"
}"""

        user_prompt = f"""Ù¾ÛŒØ§Ù… Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù†:{context_text}

Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯:
"{message}"

Ø¢ÛŒØ§ Ø§ÛŒÙ† ÛŒÚ© Ø¨Ø­Ø« Ù…Ø§Ù„ÛŒ Ø§Ø³ØªØŸ"""

        response = await self._client.chat.completions.create(
            model=self._model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.1,
            max_tokens=300,
            response_format={"type": "json_object"},
        )
        
        content = response.choices[0].message.content or "{}"
        
        try:
            data = json.loads(content)
        except json.JSONDecodeError as e:
            logger.error(
                "financial_topic_detector:json_parse_error",
                extra={"content": content, "error": str(e)},
            )
            raise
        
        return FinancialDetectionResult(
            is_financial=data.get("is_financial", False),
            topic_summary=data.get("topic_summary", ""),
            amount=data.get("amount"),
            urgency=data.get("urgency", "normal"),
            confidence=float(data.get("confidence", 0.0)),
            reason=data.get("reason", "llm_analysis"),
        )

    async def _llm_check_continuation(
        self,
        message: str,
        thread: "FinancialThread",
        recent_messages: List["FinancialThreadMessage"],
    ) -> ThreadContinuationResult:
        
        thread_context = f"Ù…ÙˆØ¶ÙˆØ¹ thread: {thread.topic_summary}\n\n"
        thread_context += "Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹:\n"
        for msg in recent_messages[-5:]:
            author = "Ú©Ø§Ø±Ø¨Ø±" if msg.author_type == "sender" else "ØµØ§Ø­Ø¨ Ø­Ø³Ø§Ø¨"
            thread_context += f"- {author}: {msg.message}\n"
        
        system_prompt = """ØªÙˆ ÛŒÚ© ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù…Ú©Ø§Ù„Ù…Ù‡ Ù‡Ø³ØªÛŒ. ÛŒÚ© thread Ù…Ø§Ù„ÛŒ ÙØ¹Ø§Ù„ Ø¯Ø§Ø±ÛŒÙ… Ùˆ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø¨Ø¯Ø§Ù†ÛŒÙ…:
1. Ø¢ÛŒØ§ Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø§Ù„ÛŒ Ø§Ø³ØªØŸ
2. Ø¢ÛŒØ§ Ù¾ÛŒØ§Ù… Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù¾Ø§ÛŒØ§Ù† Ù…ÙˆØ¶ÙˆØ¹ Ø§Ø³ØªØŸ

Ø¹Ù„Ø§Ø¦Ù… Ø§Ø¯Ø§Ù…Ù‡ Ù…ÙˆØ¶ÙˆØ¹:
âœ… Ø¬ÙˆØ§Ø¨ Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ù‚Ø¨Ù„ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…Ø¨Ù„ØºØŒ Ø²Ù…Ø§Ù†ØŒ Ø´Ù…Ø§Ø±Ù‡ Ú©Ø§Ø±Øª Ùˆ...
âœ… ØªØ£ÛŒÛŒØ¯ ÛŒØ§ Ø±Ø¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…Ø§Ù„ÛŒ
âœ… Ø³ÙˆØ§Ù„ Ø¨ÛŒØ´ØªØ± Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù‡Ù…ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹
âœ… Ø§Ø±Ø³Ø§Ù„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§Ù†Ú©ÛŒ

Ø¹Ù„Ø§Ø¦Ù… Ù¾Ø§ÛŒØ§Ù† Ù…ÙˆØ¶ÙˆØ¹:
âœ… "Ù…Ù…Ù†ÙˆÙ†"ØŒ "Ø¯Ø³ØªØª Ø¯Ø±Ø¯ Ù†Ú©Ù†Ù‡"
âœ… "Ù¾ÙˆÙ„ Ø±Ø³ÛŒØ¯"ØŒ "Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±Ø¯Ù…"
âœ… "Ø¨Ø§Ø´Ù‡ Ø¯ÛŒÚ¯Ù‡ Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§Ù…"ØŒ "Ù…Ù†ØµØ±Ù Ø´Ø¯Ù…"
âœ… "ÙØ¹Ù„Ø§Ù‹ Ø¨ÛŒâ€ŒØ®ÛŒØ§Ù„"ØŒ "Ø¨Ø¹Ø¯Ø§Ù‹ ØµØ­Ø¨Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…"
âœ… ØªØ£ÛŒÛŒØ¯ Ù†Ù‡Ø§ÛŒÛŒ (Ù…Ø«Ù„Ø§Ù‹ Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø±Ø³Ø§Ù„ Ø´Ù…Ø§Ø±Ù‡ Ú©Ø§Ø±Øª Ùˆ ØªØ£ÛŒÛŒØ¯ ÙˆØ§Ø±ÛŒØ²)

Ø¹Ù„Ø§Ø¦Ù… Ø¹Ø¯Ù… Ø§Ø±ØªØ¨Ø§Ø·:
âŒ Ø³Ù„Ø§Ù…ØŒ Ø®ÙˆØ¨ÛŒØŒ Ú†ÛŒÚ©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒ (Ø§Ø­ÙˆØ§Ù„Ù¾Ø±Ø³ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ)
âŒ Ù…ÙˆØ¶ÙˆØ¹ Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…ØªÙØ§ÙˆØª (ÙØ±Ø¯Ø§ Ø¨Ø±ÛŒÙ… Ø¨ÛŒØ±ÙˆÙ†ØŒ Ù‡ÙˆØ§ Ø®ÙˆØ¨Ù‡)
âŒ Ø³ÙˆØ§Ù„ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ú†ÛŒØ² Ø¯ÛŒÚ¯Ø±

Ù¾Ø§Ø³Ø® ÙÙ‚Ø· Ø¨Ù‡ ÙØ±Ù…Øª JSON Ø¨Ø¯Ù‡:
{
    "is_continuation": true/false,
    "is_closure": true/false,
    "confidence": 0.0-1.0,
    "reason": "Ø¯Ù„ÛŒÙ„ Ú©ÙˆØªØ§Ù‡"
}"""

        user_prompt = f"""{thread_context}
Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯:
"{message}"

Ø¢ÛŒØ§ Ø§ÛŒÙ† Ù¾ÛŒØ§Ù… Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø§Ù„ÛŒ Ø§Ø³ØªØŸ Ø¢ÛŒØ§ Ù¾Ø§ÛŒØ§Ù† Ù…ÙˆØ¶ÙˆØ¹ Ø§Ø³ØªØŸ"""

        response = await self._client.chat.completions.create(
            model=self._model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.1,
            max_tokens=200,
            response_format={"type": "json_object"},
        )
        
        content = response.choices[0].message.content or "{}"
        
        try:
            data = json.loads(content)
        except json.JSONDecodeError as e:
            logger.error(
                "financial_topic_detector:continuation_json_error",
                extra={"content": content, "error": str(e)},
            )
            raise
        
        return ThreadContinuationResult(
            is_continuation=data.get("is_continuation", False),
            is_closure=data.get("is_closure", False),
            confidence=float(data.get("confidence", 0.0)),
            reason=data.get("reason", "llm_analysis"),
        )


    async def generate_acknowledgment(
        self,
        topic_summary: str,
        creator_name: Optional[str],
        language: str = "fa",
    ) -> str:
        name_part = creator_name if creator_name else "Ø§ÛŒØ´Ø§Ù†"
        
        if language == "fa":
            return f"Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø§Ù„ÛŒÙ‡ØŒ Ø¨Ø§ÛŒØ¯ Ø§Ø² {name_part} Ø¨Ù¾Ø±Ø³Ù…. Ø¨Ù‡ Ù…Ø­Ø¶ Ø¬ÙˆØ§Ø¨ Ø¨Ù‡Øª Ù…ÛŒâ€ŒÚ¯Ù… Ú†ÛŒ Ú¯ÙØª. ğŸ’°"
        else:
            return f"This is a financial matter. I need to check with {name_part}. I'll let you know their response. ğŸ’°"

    async def generate_pending_response(
        self,
        topic_summary: str,
        creator_name: Optional[str],
        language: str = "fa",
    ) -> str:
        name_part = creator_name if creator_name else "Ø§ÛŒØ´Ø§Ù†"
        
        if language == "fa":
            return f"Ù‡Ù†ÙˆØ² {name_part} Ø¬ÙˆØ§Ø¨ Ù†Ø¯Ø§Ø¯Ù‡. Ø¨Ù‡ Ù…Ø­Ø¶ Ø¬ÙˆØ§Ø¨ Ø¨Ù‡Øª Ù…ÛŒâ€ŒÚ¯Ù…. â³"
        else:
            return f"{name_part} hasn't responded yet. I'll let you know as soon as they do. â³"

    async def generate_delivery_message(
        self,
        creator_response: str,
        topic_summary: str,
        creator_name: Optional[str],
        language: str = "fa",
    ) -> str:
        name_part = creator_name if creator_name else "Ø§ÛŒØ´Ø§Ù†"
        
        if language == "fa":
            return f"ğŸ’° Ø¯Ø±Ø¨Ø§Ø±Ù‡ {topic_summary}:\n{name_part} Ú¯ÙØª: {creator_response}"
        else:
            return f"ğŸ’° About {topic_summary}:\n{name_part} said: {creator_response}"
